{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.interpolate import BSpline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x, get_d=False):\n",
    "    return x * (x > 0) if not get_d else 1.0 * (x >= 0)\n",
    "\n",
    "def tanh(x, get_d=False):\n",
    "    return math.tanh(x) if not get_d else 1 - math.tanh(x) ** 2\n",
    "\n",
    "def sigmoid(x, get_d=False):\n",
    "    return 1 / (1 + math.exp(-x)) if not get_d else sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def get_bsplines(x_bounds, n_fun, degree=3, **kwargs):\n",
    "    grid_len = n_fun - degree + 1\n",
    "    step = (x_bounds[1] - x_bounds[0]) / (grid_len - 1)\n",
    "    edge_fun, edge_fun_d = {}, {}\n",
    "\n",
    "    #SiLU bias function\n",
    "    edge_fun[0] = lambda x: x / (1 + np.exp(-x))\n",
    "    edge_fun_d[0] = lambda x: (1 + np.exp(-x) + x * np.exp(-x)) / np.power((1 + np.exp(-x)), 2)\n",
    "    t = np.linspace(x_bounds[0] - degree * step, x_bounds[1] + degree * step, grid_len + 2 * degree)\n",
    "    t[degree] = x_bounds[0]\n",
    "    t[- degree - 1] = x_bounds[1]\n",
    "\n",
    "    for i_spline in range(n_fun - 1):\n",
    "        edge_fun[i_spline + 1] = BSpline.basis_element(t[i_spline: i_spline + degree + 2], extrapolate=False)\n",
    "        edge_fun_d[i_spline + 1] = edge_fun[i_spline + 1].derivative()\n",
    "\n",
    "    return edge_fun, edge_fun_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, n_in, n_w_per_edge, w_range=None):\n",
    "        self.n_in = n_in\n",
    "        self.n_w_per_edge = n_w_per_edge\n",
    "        w_range = [-1, 1] if w_range is None else w_range\n",
    "        self.w = np.random.uniform(w_range[0], w_range[-1], size=(self.n_in, self.n_w_per_edge))\n",
    "        self.b = 0\n",
    "\n",
    "        self.xin = None\n",
    "        self.xmid = None\n",
    "        self.xout = None\n",
    "\n",
    "        self.dxout_dxmid = None\n",
    "        self.dxout_db = None\n",
    "\n",
    "        self.dxmid_dw = None\n",
    "        self.dxmid_dxin = None\n",
    "\n",
    "        self.dxout_dw = None\n",
    "        self.dxout_dxin = None\n",
    "\n",
    "        self.dl_dw = np.zeros((self.n_in, self.n_w_per_edge))\n",
    "        self.dl_db = 0\n",
    "\n",
    "    def __call__(self, xin):\n",
    "        self.xin = np.array(xin)\n",
    "        self.get_xmid()\n",
    "        self.get_xout()\n",
    "\n",
    "        self.get_dxout_dxmid()\n",
    "        self.get_dxout_db()\n",
    "        self.get_dxmid_dw()\n",
    "        self.get_dxmid_dxin()\n",
    "\n",
    "        assert self.dxout_dxmid.shape == (self.n_in, )\n",
    "        assert self.dxmid_dxin.shape == (self.n_in, )\n",
    "        assert self.dxmid_dw.shape == (self.n_in, self.n_w_per_edge)\n",
    "\n",
    "        self.get_dxout_dxin()\n",
    "        self.get_dxout_dw()\n",
    "\n",
    "        return self.xout\n",
    "\n",
    "    def get_xmid(self):\n",
    "        pass\n",
    "\n",
    "    def get_xout(self):\n",
    "        pass\n",
    "\n",
    "    def get_dxout_dxmid(self):\n",
    "        pass\n",
    "\n",
    "    def get_dxout_db(self):\n",
    "        pass\n",
    "\n",
    "    def get_dxmid_dw(self):\n",
    "        pass\n",
    "\n",
    "    def get_dxmid_dxin(self):\n",
    "        pass\n",
    "\n",
    "    def get_dxout_dw(self):\n",
    "        self.dxout_dw = np.diag(self.dxout_dxmid) @ self.dxmid_dw\n",
    "\n",
    "    def get_dxout_dxin(self):\n",
    "        self.dxout_dxin = self.dxout_dxmid * self.dxmid_dxin\n",
    "\n",
    "    def update_dl_dw_db(self, dl_dxout):\n",
    "        self.dl_dw += self.dxout_dw * dl_dxout\n",
    "        self.dl_db += self.dxout_db * dl_dxout\n",
    "\n",
    "    def grad(self, lr):\n",
    "        self.w -= lr * self.dl_dw\n",
    "        self.b -= lr * self.dl_db\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronNN(Neuron):\n",
    "    def __init__(self, n_in, w_range=None, activation=relu):\n",
    "        super().__init__(n_in, n_w_per_edge=1, w_range=w_range)\n",
    "        self.activation = activation\n",
    "        self.activation_input = None\n",
    "\n",
    "    def get_xmid(self):\n",
    "        self.xmid = self.w[:, 0] * self.xin\n",
    "\n",
    "    def get_xout(self):\n",
    "        self.activation_input = sum(self.xmid.flatten()) + self.b\n",
    "        self.xout = self.activation(self.activation_input, get_d=False)\n",
    "\n",
    "    def get_dxout_dxmid(self):\n",
    "        self.dxout_dxmid = self.activation(self.activation_input, get_d=True) * np.ones(self.n_in)\n",
    "\n",
    "    def get_dxout_db(self):\n",
    "        self.dxout_db = self.activation(self.activation_input, get_d=True)\n",
    "\n",
    "    def get_dxmid_dw(self):\n",
    "        self.dxmid_dw = np.reshape(self.xin, (-1, 1))\n",
    "\n",
    "    def get_dxmid_dxin(self):\n",
    "        self.dxmid_dxin = self.w.flatten()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronKAN(Neuron):\n",
    "\n",
    "    def __init__(self, n_in, n_w_per_edge, x_bounds, w_range=None, get_edge_f=get_bsplines, **kwargs):\n",
    "        self.x_bounds = x_bounds\n",
    "        super().__init__(n_in, n_w_per_edge, w_range)\n",
    "        self.edge_f, self.edge_f_d = get_edge_f(x_bounds, n_w_per_edge, **kwargs)\n",
    "\n",
    "    def get_xmid(self):\n",
    "        # apply edge functions\n",
    "        self.phi_x_mat = np.array([self.edge_f[b](self.xin) for b in self.edge_f]).T\n",
    "        self.phi_x_mat[np.isnan(self.phi_x_mat)] = 0\n",
    "        self.xmid = (self.w * self.phi_x_mat).sum(axis=1)\n",
    "\n",
    "    def get_xout(self):\n",
    "        # use tanh as activation function to avoid any updates of the spline grids\n",
    "        self.xout = tanh(sum(self.xmid.flatten()), get_d=False)\n",
    "\n",
    "    def get_dxout_dxmid(self):\n",
    "        self.dxout_dxmid = tanh(sum(self.xmid.flatten()), get_d=True) * np.ones(self.n_in)\n",
    "\n",
    "    def get_dxout_db(self):\n",
    "        self.dxout_db = 0\n",
    "\n",
    "    def get_dxmid_dw(self):\n",
    "        self.dxmid_dw = self.phi_x_mat\n",
    "\n",
    "    def get_dxmid_dxin(self):\n",
    "        phi_x_d_mat = np.array([\n",
    "            self.edge_f_d[b](self.xin)\n",
    "            if self.edge_f_d[b](self.xin) is not None\n",
    "            else 0 \n",
    "            for b in self.edge_f\n",
    "        ]).T # shape (n_in, n_w_per_edge)\n",
    "        phi_x_d_mat[np.isnan(phi_x_d_mat)] = 0\n",
    "\n",
    "        self.dxmid_dxin = (self.w * phi_x_d_mat).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "\n",
    "    def __init__(self, n_in, n_out, neuron_class=NeuronNN, **kwargs) -> None:\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.neurons = [neuron_class(n_in) if (kwargs == {}) else neuron_class(n_in, **kwargs) for n in range(n_out)]\n",
    "        self.xin = None # input; shape (n_in, )\n",
    "        self.xout = None # output; shape (n_out, )\n",
    "        self.dl_dxin = None # dloss/dxin; shape (n_in, )\n",
    "        self.zero_grad()\n",
    "\n",
    "    def __call__(self, xin) -> Any:\n",
    "        self.xin = xin\n",
    "        self.xout = np.array([n(self.xin) for n in self.neurons])\n",
    "        return self.xout\n",
    "\n",
    "    def zero_grad(self, which=None):\n",
    "        if which is None:\n",
    "            which = ['xin', 'weights', 'bias']\n",
    "        for w in which:\n",
    "            if w == 'xin':\n",
    "                self.dl_dxin = np.zeros(self.n_in)\n",
    "            elif w == 'weights':\n",
    "                for n in self.neurons:\n",
    "                    n.dl_dw = np.zeros((self.n_in, self.neurons[0].n_w_per_edge))\n",
    "            elif w == 'bias':\n",
    "                for n in self.neurons:\n",
    "                    n.dl_db = 0\n",
    "            else:\n",
    "                raise ValueError('input \\'which\\' value not recognized')\n",
    "\n",
    "    def update_grad(self, dl_dxout):\n",
    "        for i, dl_dxout_tmp in enumerate(dl_dxout):\n",
    "            self.dl_dxin += self.neurons[i].dxout_dxin  * dl_dxout_tmp\n",
    "            self.neurons[i].update_dl_dw_db(dl_dxout_tmp)\n",
    "        return self.dl_dxin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Loss:\n",
    "\n",
    "    def __init__(self, n_in) -> None:\n",
    "        self.n_in = n_in\n",
    "        self.y, self.dl_dy, self.loss, self.y_train = None, None, None, None\n",
    "\n",
    "    def __call__(self, y, y_train) -> Any:\n",
    "        # y: output of the net\n",
    "        # y_train: ground truth\n",
    "        self.y = np.array(y)\n",
    "        self.y_train = y_train\n",
    "        self.get_loss()\n",
    "        self.get_dl_dy()\n",
    "        return self.loss\n",
    "\n",
    "    def get_loss(self):\n",
    "        pass\n",
    "\n",
    "    def get_dl_dy(self):\n",
    "        pass\n",
    "\n",
    "class SquaredLoss(Loss):\n",
    "    def get_loss(self):\n",
    "        self.loss = np.mean(np.power(self.y - self.y_train, 2))\n",
    "\n",
    "    def get_dl_dy(self):\n",
    "        self.dl_dy = 2 * (self.y - self.y_train) / self.n_in\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def get_loss(self):\n",
    "        self.loss = - np.log(self.y[self.y_train[0]]) / sum(np.exp(self.y))\n",
    "\n",
    "    def get_dl_dy(self):\n",
    "        self.dl_dy = np.exp(self.y) / sum(np.exp(self.y))\n",
    "        self.dl_dy[self.y_train] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "\n",
    "    def __init__(self, layer_len, eps=.0001, seed=None, loss=SquaredLoss, **kwargs) -> None:\n",
    "        self.seed = np.random.randint(int(1e4)) if seed is None else int(seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        self.layer_len = layer_len\n",
    "        self.eps = eps\n",
    "        self.n_layers = len(self.layer_len) - 1\n",
    "        self.layers = [FullyConnectedLayer(self.layer_len[i], self.layer_len[i + 1], **kwargs) for i in range(self.n_layers)]\n",
    "        self.loss = loss(self.layer_len[-1])\n",
    "        self.loss_hist = None\n",
    "\n",
    "    def __call__(self, x) -> Any:\n",
    "        xin = x\n",
    "        for l in range(self.n_layers):\n",
    "            xin = self.layers[l](xin)\n",
    "\n",
    "        return xin\n",
    "    \n",
    "    def backprop(self):\n",
    "        delta = self.layers[-1].update_grad(self.loss.dl_dy)\n",
    "        for l in range(self.n_layers - 1)[::-1]:\n",
    "            delta = self.layers[l].update_grad(delta)\n",
    "\n",
    "    def grad_desc_par(self):\n",
    "        for l in self.layers:\n",
    "            for n in l.neurons:\n",
    "                n.grad(self.eps)\n",
    "\n",
    "    def train(self, x_train, y_train, n_iter_max=10000, loss_tol=.1):\n",
    "        self.loss_hist = np.zeros(n_iter_max)\n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "        assert x_train.shape[0] == y_train.shape[0], 'x_train & y_train must have the same number of samples'\n",
    "        assert x_train.shape[1] == self.layer_len[0], 'shape of x_train & first layer are incompatible'\n",
    "\n",
    "        pbar = tqdm(range(n_iter_max))\n",
    "\n",
    "        for it in pbar:\n",
    "            loss = 0\n",
    "            for i in range(x_train.shape[0]):\n",
    "                xout = self(x_train[i])\n",
    "                loss += self.loss(xout, y_train[i, :])\n",
    "                self.backprop()\n",
    "                [l.zero_grad(which=['xin']) for l in self.layers]\n",
    "            self.loss_hist[it] = loss\n",
    "            if (it % 10 == 0):\n",
    "                pbar.set_postfix_str(f'loss: {loss:.3f}')\n",
    "            if loss < loss_tol:\n",
    "                pbar.set_postfix_str(f'loss: {loss:.3f} | Convergence has been attained')\n",
    "                self.loss_hist = self.loss_hist[:it]\n",
    "                break\n",
    "            self.grad_desc_par()\n",
    "            [l.zero_grad(which=['weights', 'bias']) for l in self.layers]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter_train_1d = 500\n",
    "loss_tol_1d = .05\n",
    "seed = 476\n",
    "\n",
    "x_train = np.linspace(-1, 1, 50).reshape(-1, 1)\n",
    "y_train  = .5 * np.sin(4 * x_train) * np.exp(-(x_train + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 302/500 [00:05<00:03, 59.04it/s, loss: 1.085]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[218], line 22\u001b[0m\n\u001b[1;32m     13\u001b[0m mlp1d \u001b[38;5;241m=\u001b[39m FeedForward([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m     14\u001b[0m                     eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.005\u001b[39m, \n\u001b[1;32m     15\u001b[0m                     seed\u001b[38;5;241m=\u001b[39mseed, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m                     w_range\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m.5\u001b[39m],\n\u001b[1;32m     19\u001b[0m                 )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# kan1d.train(x_train, y_train, n_iter_train_1d, loss_tol_1d)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmlp1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter_train_1d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_tol_1d\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[209], line 45\u001b[0m, in \u001b[0;36mFeedForward.train\u001b[0;34m(self, x_train, y_train, n_iter_max, loss_tol)\u001b[0m\n\u001b[1;32m     43\u001b[0m     xout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x_train[i])\n\u001b[1;32m     44\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(xout, y_train[i, :])\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     [l\u001b[38;5;241m.\u001b[39mzero_grad(which\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxin\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_hist[it] \u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[209], line 24\u001b[0m, in \u001b[0;36mFeedForward.backprop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate_grad(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mdl_dy)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 24\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[207], line 38\u001b[0m, in \u001b[0;36mFullyConnectedLayer.update_grad\u001b[0;34m(self, dl_dxout)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dl_dxout_tmp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dl_dxout):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl_dxin \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[i]\u001b[38;5;241m.\u001b[39mdxout_dxin  \u001b[38;5;241m*\u001b[39m dl_dxout_tmp\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[i]\u001b[38;5;241m.\u001b[39mupdate_dl_dw_db(dl_dxout_tmp)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl_dxin\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# kan1d = FeedForward([1, 2, 2, 1], \n",
    "#                     eps=.01, \n",
    "#                     seed=seed, \n",
    "#                     neuron_class=NeuronKAN, \n",
    "#                     n_w_per_edge=7, \n",
    "#                     x_bounds=[-1, 1],\n",
    "#                     get_edge_f=get_bsplines,\n",
    "#                     w_range=[-1, 1],\n",
    "#                 )\n",
    "\n",
    "mlp1d = FeedForward([1, 13, 1], \n",
    "                    eps=.005, \n",
    "                    seed=seed, \n",
    "                    neuron_class=NeuronNN, \n",
    "                    activation=relu, \n",
    "                    w_range=[-.5, .5],\n",
    "                )\n",
    "\n",
    "# kan1d.train(x_train, y_train, n_iter_train_1d, loss_tol_1d)\n",
    "mlp1d.train(x_train, y_train, n_iter_train_1d, loss_tol_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun2d(X1, X2):\n",
    "    return X1 * np.power(X2, .5)\n",
    " \n",
    "X1, X2 = np.meshgrid(np.linspace(0, .8, 8), np.linspace(0, 1, 10))\n",
    "Y_training = fun2d(X1, X2)\n",
    " \n",
    "x_train2d = np.concatenate((X1.reshape(-1, 1), X2.reshape(-1, 1)), axis=1)\n",
    "y_train2d = Y_training.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 325/500 [00:09<00:05, 32.81it/s, loss: 0.464]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[212], line 22\u001b[0m\n\u001b[1;32m      4\u001b[0m kan2d \u001b[38;5;241m=\u001b[39m FeedForward([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m      5\u001b[0m                     eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.01\u001b[39m, \n\u001b[1;32m      6\u001b[0m                     seed\u001b[38;5;241m=\u001b[39mseed, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m                     w_range\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     12\u001b[0m                 )\n\u001b[1;32m     14\u001b[0m mlp2d \u001b[38;5;241m=\u001b[39m FeedForward([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m     15\u001b[0m                     eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.01\u001b[39m, \n\u001b[1;32m     16\u001b[0m                     seed\u001b[38;5;241m=\u001b[39mseed, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m                     w_range\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m.5\u001b[39m],\n\u001b[1;32m     20\u001b[0m                 )\n\u001b[0;32m---> 22\u001b[0m \u001b[43mkan2d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter_train_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_tol_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# mlp2d.train(x_train2d, y_train2d, n_iter_train_2d, loss_tol_2d)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[209], line 43\u001b[0m, in \u001b[0;36mFeedForward.train\u001b[0;34m(self, x_train, y_train, n_iter_max, loss_tol)\u001b[0m\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 43\u001b[0m     xout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(xout, y_train[i, :])\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackprop()\n",
      "Cell \u001b[0;32mIn[209], line 17\u001b[0m, in \u001b[0;36mFeedForward.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m xin \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[0;32m---> 17\u001b[0m     xin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xin\n",
      "Cell \u001b[0;32mIn[207], line 17\u001b[0m, in \u001b[0;36mFullyConnectedLayer.__call__\u001b[0;34m(self, xin)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, xin) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxin \u001b[38;5;241m=\u001b[39m xin\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxout \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneurons\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxout\n",
      "Cell \u001b[0;32mIn[207], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, xin) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxin \u001b[38;5;241m=\u001b[39m xin\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxout \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxin\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons])\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxout\n",
      "Cell \u001b[0;32mIn[204], line 34\u001b[0m, in \u001b[0;36mNeuron.__call__\u001b[0;34m(self, xin)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dxout_db()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dxmid_dw()\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dxmid_dxin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdxout_dxmid\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_in, )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdxmid_dxin\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_in, )\n",
      "Cell \u001b[0;32mIn[206], line 28\u001b[0m, in \u001b[0;36mNeuronKAN.get_dxmid_dxin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dxmid_dxin\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     phi_x_d_mat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_f_d\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_f_d\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_f\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;66;03m# shape (n_in, n_w_per_edge)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     phi_x_d_mat[np\u001b[38;5;241m.\u001b[39misnan(phi_x_d_mat)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdxmid_dxin \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m*\u001b[39m phi_x_d_mat)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[206], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dxmid_dxin\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     28\u001b[0m     phi_x_d_mat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_f_d[b](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxin)\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_f_d\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxin\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_f\n\u001b[1;32m     33\u001b[0m     ])\u001b[38;5;241m.\u001b[39mT \u001b[38;5;66;03m# shape (n_in, n_w_per_edge)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     phi_x_d_mat[np\u001b[38;5;241m.\u001b[39misnan(phi_x_d_mat)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdxmid_dxin \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m*\u001b[39m phi_x_d_mat)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scipy/interpolate/_bsplines.py:503\u001b[0m, in \u001b[0;36mBSpline.__call__\u001b[0;34m(self, x, nu, extrapolate)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_c_contiguous()\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(x, nu, extrapolate, out)\n\u001b[0;32m--> 503\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# transpose to move the calculated values to the interpolation axis\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mndim))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# n_iter_train_2d = 500\n",
    "# loss_tol_2d = .05\n",
    "\n",
    "# kan2d = FeedForward([2, 2, 2, 1], \n",
    "#                     eps=.01, \n",
    "#                     seed=seed, \n",
    "#                     neuron_class=NeuronKAN, \n",
    "#                     n_w_per_edge=7, \n",
    "#                     x_bounds=[-1, 1],\n",
    "#                     get_edge_f=get_bsplines,\n",
    "#                     w_range=[-1, 1],\n",
    "#                 )\n",
    "\n",
    "# mlp2d = FeedForward([2, 13, 1], \n",
    "#                     eps=.01, \n",
    "#                     seed=seed, \n",
    "#                     neuron_class=NeuronNN, \n",
    "#                     activation=relu, \n",
    "#                     w_range=[-.5, .5],\n",
    "                # )\n",
    "\n",
    "# kan2d.train(x_train2d, y_train2d, n_iter_train_2d, loss_tol_2d)\n",
    "# mlp2d.train(x_train2d, y_train2d, n_iter_train_2d, loss_tol_2d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
