{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def relu(x, get_d=False):\n",
    "    return x * (x > 0) if not get_d else 1.0 * (x >= 0)\n",
    "\n",
    "def tanh(x, get_d=False):\n",
    "    return math.tanh(x) if not get_d else 1 - math.tanh(x) ** 2\n",
    "\n",
    "def sigmoid(x, get_d=False):\n",
    "    return 1 / (1 + math.exp(-x)) if not get_d else sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, n_in, n_w_per_edge, w_range=None):\n",
    "        self.n_in = n_in\n",
    "        self.n_w_per_edge = n_w_per_edge\n",
    "        w_range = [-1, 1] if w_range is None else w_range\n",
    "        self.w = np.random.uniform(w_range[0], w_range[-1], size=(self.n_in, self.n_w_per_edge))\n",
    "        self.b = 0\n",
    "\n",
    "        self.xin = None\n",
    "        self.xmid = None\n",
    "        self.xout = None\n",
    "\n",
    "        self.dxout_dxmid = None\n",
    "        self.dxout_db = None\n",
    "\n",
    "        self.dxmid_dw = None\n",
    "        self.dxmid_dxin = None\n",
    "\n",
    "        self.dxout_dw = None\n",
    "        self.dxout_dxin = None\n",
    "\n",
    "        self.dl_dw = np.zeros((self.n_in, self.n_w_per_edge))\n",
    "        self.dl_db = 0\n",
    "\n",
    "    def __call__(self, xin):\n",
    "        self.xin = np.array(xin)\n",
    "        self.get_xmid()\n",
    "        self.get_xout()\n",
    "\n",
    "        self.get_dxout_dxmid()\n",
    "        self.get_dxout_db()\n",
    "        self.get_dxmid_dw()\n",
    "        self.get_dmid_dxin()\n",
    "\n",
    "        assert self.dxout_dxmid.shape == (self.n_in, )\n",
    "        assert self.dxmid_dxin.shape == (self.n_in, )\n",
    "        assert self.dxmid_dw.shape == (self.n_in, self.n_w_per_edge)\n",
    "\n",
    "        self.get_dxout_dw()\n",
    "        self.get_dxout_dxin()\n",
    "\n",
    "        return self.xout\n",
    "\n",
    "    def get_xmid(self):\n",
    "        pass\n",
    "\n",
    "    def get_xout(self):\n",
    "        pass\n",
    "\n",
    "    def get_dxout_dxmid(self):\n",
    "        pass\n",
    "\n",
    "    def get_dxout_db(self):\n",
    "        pass\n",
    "\n",
    "    def get_dxmid_dw(self):\n",
    "        pass\n",
    "\n",
    "    def get_dmid_dxin(self):\n",
    "        pass\n",
    "\n",
    "    def get_dxout_dw(self):\n",
    "        self.dxout_dw = np.diag(self.dxout_dxmid) @ self.dxmid_dw\n",
    "\n",
    "    def get_dxout_dxin(self):\n",
    "        self.dxout_dxin = self.dxout_dxmid * self.dxmid_dxin\n",
    "\n",
    "    def update_dl_dw_db(self, dl_dxout):\n",
    "        self.dl_dw += self.dxout_dw * dl_dxout\n",
    "        self.dl_db += self.dxout_db * dl_dxout\n",
    "\n",
    "    def grad(self, lr):\n",
    "        self.w -= lr * self.dl_dw\n",
    "        self.b -= lr * self.dl_db\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello=np.array([[1,2,3], [4,5,6]])\n",
    "bye = np.array([0.4, 0.5])\n",
    "hello[:, 0] * bye\n",
    "\n",
    "np.reshape(hello, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronNN(Neuron):\n",
    "    def __init__(self, n_in, w_range=None, activation=relu):\n",
    "        super().__init__(self, n_in, n_w_per_edge=1, w_range=w_range)\n",
    "        self.activation = activation\n",
    "        self.activation_input = None\n",
    "\n",
    "    def get_xmid(self):\n",
    "        self.xmid = self.w[:, 0] * self.xin\n",
    "\n",
    "    def get_xout(self):\n",
    "        self.activation_input = sum(self.xmid.flatten()) + self.b\n",
    "        self.xout = self.activation(self.activation_input, get_d=False)\n",
    "\n",
    "    def get_dxout_dxmid(self):\n",
    "        self.dxout_dxmid = self.activation(self.activation_input, get_d=True) * np.ones(self.n_in)\n",
    "\n",
    "    def get_dxout_db(self):\n",
    "        self.dxout_dxmid = self.activation(self.activation_input, get_d=True)\n",
    "\n",
    "    def get_dxmid_dw(self):\n",
    "        self.dxmid_dw = np.reshape(self.xin, (-1, 1))\n",
    "\n",
    "    def get_dxmid_dxin(self):\n",
    "        self.dxmid_dxin = self.w.flatten()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import BSpline\n",
    "\n",
    "def get_bsplines(x_bounds, n_fun, degree=3, **kwargs):\n",
    "    grid_len = n_fun - degree + 1\n",
    "    step = (x_bounds[1] - x_bounds[0]) / (grid_len - 1)\n",
    "    edge_fun, edge_fun_d = {}, {}\n",
    "\n",
    "    #SiLU bias function\n",
    "    edge_fun[0] = lambda x: x / (1 + np.exp(-x))\n",
    "    edge_fun_d[0] = lambda x: (1 + np.exp(-x) + x * np.exp(-x)) / np.power((1 + np.exp(-x)), 2)\n",
    "    t = np.linspace(x_bounds[0] - degree * step, x_bounds[1] + degree * step, grid_len + 2 * degree)\n",
    "    t[degree] = x_bounds[0]\n",
    "    t[- degree - 1] = x_bounds[1]\n",
    "\n",
    "    for i_spline in range(n_fun - 1):\n",
    "        edge_fun[i_spline + 1] = BSpline.basis_element()\n",
    "        edge_fun_d[i_spline + 1] = edge_fun[i_spline + 1].derivative()\n",
    "\n",
    "    return edge_fun, edge_fun_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronKAN(Neuron):\n",
    "\n",
    "    def __init__(self, n_in, n_w_per_edge, x_bounds, w_range=None, get_edge_f=get_bsplines, **kwargs):\n",
    "        self.x_bounds = x_bounds\n",
    "        super().__init__(n_in, n_w_per_edge, w_range)\n",
    "        self.edge_f, self.edge_f_d = get_edge_f(x_bounds, n_w_per_edge, **kwargs)\n",
    "\n",
    "    def get_xmid(self):\n",
    "        # apply edge functions\n",
    "        self.phi_x_mat = np.array([self.edge_f[b](self.xin) for b in self.edge_f]).T\n",
    "        self.phi_x_mat[np.isnan(self.phi_x_mat)] = 0\n",
    "        self.xmid = (self.w * self.phi_x_mat).sum(axis=1)\n",
    "\n",
    "    def get_xout(self):\n",
    "        # use tanh as activation function to avoid any updates of the spline grids\n",
    "        self.xout = tanh(sum(self.xmid.flatten()), get_d=False)\n",
    "\n",
    "    def get_dxout_dxmid(self):\n",
    "        self.dxout_dxmid = tanh(sum(self.xmid.flatten()), get_d=True)\n",
    "\n",
    "    def get_dxout_db(self):\n",
    "        self.dxout_db = 0\n",
    "\n",
    "    def get_dxmid_dw(self):\n",
    "        self.dxmid_dw = self.phi_x_mat\n",
    "\n",
    "    def get_dxmid_dxin(self):\n",
    "        phi_x_d_mat = np.array([\n",
    "            self.edge_f_der[b](self.xin)\n",
    "            if self.edge_f_der[b](self.xin) is not None\n",
    "            else 0 \n",
    "            for b in self.edge_f\n",
    "        ]).T # shape (n_in, n_w_per_edge)\n",
    "        phi_x_d_mat[np.isnan(phi_x_d_mat)] = 0\n",
    "\n",
    "        self.dxmid_dxin = (self.w * phi_x_d_mat).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "\n",
    "    def __init__(self, n_in, n_out, neuron_class=NeuronNN, **kwargs) -> None:\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.neurons = [neuron_class(n_in) if (kwargs == {}) else neuron_class(n_in, **kwargs) for n in range(n_out)]\n",
    "        self.xin = None # input; shape (n_in, )\n",
    "        self.xout = None # output; shape (n_out, )\n",
    "        self.dl_dxin = None # dloss/dxin; shape (n_in, )\n",
    "        self.zero_grad()\n",
    "\n",
    "    def __call__(self, xin) -> Any:\n",
    "        self.xin = xin\n",
    "        self.xout = np.array([n(self.xin) for n in self.neurons])\n",
    "        return self.xout\n",
    "\n",
    "    def zero_grad(self, which=None):\n",
    "        if which is None:\n",
    "            which = ['xin', 'weights', 'bias']\n",
    "        for w in which:\n",
    "            if w == 'xin':\n",
    "                self.dl_dxin = np.zeros(self.n_in)\n",
    "            elif w == 'weights':\n",
    "                for n in self.neurons:\n",
    "                    n.dl_dw = np.zeros(self.n_in, self.neurons[0].n_w_per_edge)\n",
    "            elif w == 'bias':\n",
    "                for n in self.neurons:\n",
    "                    n.dl_db = 0\n",
    "            else:\n",
    "                raise ValueError('input \\'which\\' value not recognized')\n",
    "\n",
    "    def update_grad(self, dl_dxout):\n",
    "        for i, dl_dxout_tmp in enumerate(dl_dxout):\n",
    "            self.dl_dxin += self.neurons[i].dxout_dxin * dl_dxout_tmp\n",
    "            self.neurons[i].update_dl_dw_db(dl_dxout_tmp)\n",
    "        return self.dl_dxin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Loss:\n",
    "\n",
    "    def __init__(self, n_in) -> None:\n",
    "        self.n_in = n_in\n",
    "        self.y, self.dl_dy, self.loss, self.y_train = None, None, None, None\n",
    "\n",
    "    def __call__(self, y, y_train) -> Any:\n",
    "        # y: output of the net\n",
    "        # y_train: ground truth\n",
    "        self.y = np.array(y)\n",
    "        self.y_train = y_train\n",
    "        self.get_loss()\n",
    "        self.get_dl_dy()\n",
    "\n",
    "    def get_loss(self):\n",
    "        pass\n",
    "\n",
    "    def get_dl_dy(self):\n",
    "        pass\n",
    "\n",
    "class SquaredLoss(Loss):\n",
    "    def get_loss(self):\n",
    "        self.loss = np.mean(np.power(self.y - self.y_train, 2))\n",
    "\n",
    "    def get_dl_dy(self):\n",
    "        self.dl_dy = 2 * (self.y - self.y_train) / self.n_in\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def get_loss(self):\n",
    "        self.loss = - np.log(self.y[self.y_train[0]]) / sum(np.exp(self.y))\n",
    "\n",
    "    def get_dl_dy(self):\n",
    "        self.dl_dy = np.exp(self.y) / sum(np.exp(self.y))\n",
    "        self.dl_dy[self.y_train] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FeedForward:\n",
    "\n",
    "    def __init__(self, layer_len, eps=.0001, seed=None, loss=SquaredLoss, **kwargs) -> None:\n",
    "        self.seed = np.random.randint(int(1e4)) if seed is None else int(seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        self.layer_len = layer_len\n",
    "        self.eps = eps\n",
    "        self.n_layers = len(self.layer_len) - 1\n",
    "        self.layers = [FullyConnectedLayer(self.layer_len[i], self.layer_len[i + 1], **kwargs) for i in range(self.n_layers)]\n",
    "        self.loss = loss(self.layer_len[-1])\n",
    "        self.loss_hist = None\n",
    "\n",
    "    def __call__(self, x) -> Any:\n",
    "        xin = x\n",
    "        for l in range(self.layers):\n",
    "            xin = self.layers[l](xin)\n",
    "\n",
    "        return xin\n",
    "    \n",
    "    def backprop(self):\n",
    "        delta = self.layers[-1].update_grad(self.loss.dl_dy)\n",
    "        for l in range(self.n_layers - 1)[::-1]:\n",
    "            delta = self.layers[l].update_grad(delta)\n",
    "\n",
    "    def grad_desc_par(self):\n",
    "        for l in self.layers:\n",
    "            for n in l.neurons:\n",
    "                n.grad(self.eps)\n",
    "\n",
    "    def train(self, x_train, y_train, n_iter_max=10000, loss_tol=.1):\n",
    "        self.loss_hist = np.zeros(n_iter_max)\n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
